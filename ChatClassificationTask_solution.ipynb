{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlfJZFhBki59"
   },
   "source": [
    "# –¢–µ—Å—Ç–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –ø–æ–∑–∏—Ü–∏—é DS'–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZb-Vy_yki6A"
   },
   "source": [
    "## –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiNPJcFBki6B"
   },
   "source": [
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ç–æ–≥–æ, –∏–∑ –∫–∞–∫–æ–≥–æ —á–∞—Ç–∞ –æ–Ω–∏ –±—ã–ª–∏ –≤–∑—è—Ç—ã. –î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–æ–æ–±—â–µ–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –¥–≤—É—Ö –ø—É–±–ª–∏—á–Ω—ã—Ö —á–∞—Ç–æ–≤:\n",
    "* –ß–∞—Ç –ø–æ Python (label=0)\n",
    "* –ß–∞—Ç –ø–æ Data Science (label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiAb4Bmki6B"
   },
   "source": [
    "## –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QKiD80aki6B"
   },
   "source": [
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞:\n",
    "1. –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–æ–≤\n",
    "2. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –º–µ—Ç—Ä–∏–∫–∏.\n",
    "3. –û–±—ä—è—Å–Ω–∏—Ç—å –∏—Ö –≤—ã–±–æ—Ä (–¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞/–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏). \n",
    "4. –°–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –ø–æ —Ä–∞–±–æ—Ç–µ –º–æ–¥–µ–ª–∏(–µ–π) (–¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞/–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏), –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–±–ª–µ–º—ã, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤—ã –∏–ª–∏ –≤–∞—à–∞ –º–æ–¥–µ–ª—å —Å—Ç–æ–ª–∫–Ω—É–ª–∞—Å—å.\n",
    "\n",
    "–ë—ã–ª–æ –±—ã –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –Ω–æ –∏ –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (–¥–∞–∂–µ –ø—Ä–æ–≤–∞–ª—å–Ω—ã–µ). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ozF76zmki6G"
   },
   "source": [
    "–ò—Ç–æ–≥–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–∞–ª–µ–µ –Ω—É–∂–Ω–æ:\n",
    "1. –û–±–µ—Ä–Ω—É—Ç—å –≤ –∫–ª–∞—Å—Å\n",
    "2. –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∫ REST API\n",
    "3. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ–±—ã —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å –∞–ø–∏ –º–æ–¥–µ–ª–∏ —É —Å–µ–±—è –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7skFhwFEki6C"
   },
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JaxKb7yjlrgc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "ZXcV3yiLki6C",
    "outputId": "6ce47555-fdcc-4583-fb6e-de13b3949f47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/bhwblzw50zq_dtfl9p7kdbq00000gn/T/ipykernel_8296/4054579702.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7729</th>\n",
       "      <td>–ê –Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥ –≤–æ–ø—Ä–æ—Å —Å—Ç–æ—è–ª –∫–∞–∫ \"–ø—Ä–æ–±—å—é—Ç 1.25 –¥–æ –∫–æ–Ω—Ü–∞ –∫–æ–Ω–∫—É—Ä—Å–∞ –∏–ª–∏ –Ω–µ—Ç?\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12058</th>\n",
       "      <td>–Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ –Ω–µ –Ω—É–∂–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞. –û–Ω –≤–∑–∞–∏–º–æ–µ–π—Å—Ç–≤–∏–µ—Ç —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –∏–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ —Å–µ—Ç—å. –°—Ç–∞–ª–æ –±—ã—Ç—å –µ—â–µ –∏ –æ—Å–æ–±—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –Ω–∞–∂–µ–¥–Ω–æ—Å—Ç–∏ —Å–µ—Ç–∏....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>—Å–∏—Å—Ç–µ–º–µ. —Ä–æ–±–æ—Ç—É. –±–µ–∑–¥—É—à–Ω–æ–π –º–∞—à–∏–Ω–µ.  –°–∏—Å—Ç–µ–º–µ, –µ—Å–ª–∏ —É–≥–æ–¥–Ω–æ, –∫–æ—Ç–æ—Ä–æ–π –ø–æ—Ñ–∏–≥ –Ω–∞ –∫–∞–∂–¥–æ–≥–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –º—ã - —é–∑–µ—Ä—ã - –Ω–∏—á—Ç–æ –≤ –æ–∫–µ–∞–Ω–µ.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>–ø–µ—Ä–∏–æ–¥ —ç–∫–≥ == –æ–¥–∏–Ω —É–¥–∞—Ä —Å–µ—Ä–¥—Ü–∞, –µ—Å–ª–∏ —è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—è–ª</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>–ê –≤—ã –Ω–µ –¥—É–º–∞–ª–∏, —á—Ç–æ –æ–Ω –º–æ–≥ –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π üåö</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   text  \\\n",
       "7729   –ê –Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥ –≤–æ–ø—Ä–æ—Å —Å—Ç–æ—è–ª –∫–∞–∫ \"–ø—Ä–æ–±—å—é—Ç 1.25 –¥–æ –∫–æ–Ω—Ü–∞ –∫–æ–Ω–∫—É—Ä—Å–∞ –∏–ª–∏ –Ω–µ—Ç?\"                                                                          \n",
       "12058  –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ –Ω–µ –Ω—É–∂–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞. –û–Ω –≤–∑–∞–∏–º–æ–µ–π—Å—Ç–≤–∏–µ—Ç —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –∏–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ —Å–µ—Ç—å. –°—Ç–∞–ª–æ –±—ã—Ç—å –µ—â–µ –∏ –æ—Å–æ–±—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –Ω–∞–∂–µ–¥–Ω–æ—Å—Ç–∏ —Å–µ—Ç–∏....   \n",
       "3757   —Å–∏—Å—Ç–µ–º–µ. —Ä–æ–±–æ—Ç—É. –±–µ–∑–¥—É—à–Ω–æ–π –º–∞—à–∏–Ω–µ.  –°–∏—Å—Ç–µ–º–µ, –µ—Å–ª–∏ —É–≥–æ–¥–Ω–æ, –∫–æ—Ç–æ—Ä–æ–π –ø–æ—Ñ–∏–≥ –Ω–∞ –∫–∞–∂–¥–æ–≥–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –º—ã - —é–∑–µ—Ä—ã - –Ω–∏—á—Ç–æ –≤ –æ–∫–µ–∞–Ω–µ.                     \n",
       "3598   –ø–µ—Ä–∏–æ–¥ —ç–∫–≥ == –æ–¥–∏–Ω —É–¥–∞—Ä —Å–µ—Ä–¥—Ü–∞, –µ—Å–ª–∏ —è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—è–ª                                                                                              \n",
       "1538   –ê –≤—ã –Ω–µ –¥—É–º–∞–ª–∏, —á—Ç–æ –æ–Ω –º–æ–≥ –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π üåö                                                                                                   \n",
       "\n",
       "       label  \n",
       "7729   1      \n",
       "12058  0      \n",
       "3757   1      \n",
       "3598   1      \n",
       "1538   1      "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDCETiU4ki6E",
    "outputId": "1cbf10a4-8bf2-428d-8376-060f569c9f43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12404, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIysKbhiki6F"
   },
   "source": [
    "# –í–∞—à–µ —Ä–µ—à–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3Zza0NplAC5",
    "outputId": "e72352fc-141b-4e7b-9784-6b664f04c1c9"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install nltk\n",
    "# ! pip3 install pymorphy2\n",
    "# ! pip3 install stop-words\n",
    "# ! pip3 install pymystem3\n",
    "# ! pip3 install catboost\n",
    "# ! pip3 install gensim\n",
    "# ! pip3 install flask\n",
    "# ! pip3 install flask_restful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrBB44_a8Cdm"
   },
   "source": [
    "## –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jqtr6v9x1K4T",
    "outputId": "b82ab660-50b9-4c30-a479-5653d081b373"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ilya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "_create_unverified_https_context = ssl._create_unverified_context\n",
    "ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EpQ-017j1PIt"
   },
   "outputs": [],
   "source": [
    "# –∑–∞–≥–æ—Ç–æ–≤–∏–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "russian_stopwords_2 = get_stop_words('russian')\n",
    "\n",
    "russian_stopwords  = list(set(russian_stopwords) | set(russian_stopwords_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PU9Rtyrqki6E"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# —É–¥–∞–ª–∏–º –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —á–∏—Å–ª–∞\n",
    "deleted_symols = punctuation + '0123456789'\n",
    "func = lambda text: ''.join([char for char in text if str(char) not in deleted_symols])\n",
    "df['text_prepared'] = df['text'].apply(func)\n",
    "\n",
    "# —É–¥–∞–ª–∏–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "func = lambda text: ' '.join([word for word in text.split(' ') if word not in russian_stopwords])\n",
    "df['text_prepared'] = df['text_prepared'].str.lower().apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "AZqz_f1N3M98",
    "outputId": "522456a2-78dd-4e25-bd1a-62e609a4e80b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_prepared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–µ—Å–ª–∏ —É —Ç–µ–±—è –Ω–µ—Ç –≤–µ—Ä—Ö–Ω–µ–≥–æ –ø—Ä–µ–¥–µ–ª–∞, —Ñ–∏–∫—Å–µ–¥ –ø–æ–∏–Ω—Ç —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞</td>\n",
       "      <td>1</td>\n",
       "      <td>–≤–µ—Ä—Ö–Ω–µ–≥–æ –ø—Ä–µ–¥–µ–ª–∞ —Ñ–∏–∫—Å–µ–¥ –ø–æ–∏–Ω—Ç –ø—Ä–æ–±–ª–µ–º–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ê–≥–∞ –∏–ª–∏ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª–∞ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –∏ –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td>\n",
       "      <td>0</td>\n",
       "      <td>–∞–≥–∞ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª–∞ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—è –¥—É–º–∞—é, —á—Ç–æ —Å–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∑–æ–∫ –∫ –º–∞–∫—Å–∏–º—É–º—É.</td>\n",
       "      <td>1</td>\n",
       "      <td>–¥—É–º–∞—é —Å–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±–ª–∏–∑–æ–∫ –º–∞–∫—Å–∏–º—É–º—É</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢—ã –¥–µ–ª–∞–µ—à—å —Ç—É –∑–∞–¥–∞—á—É —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º 20–∫ —Å—Ç—Ä–∞–Ω–∏—Ü?) –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏ –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ —É–∂–µ –±—ã —Å–ø–∞—Ä—Å–∏–ª)</td>\n",
       "      <td>0</td>\n",
       "      <td>–¥–µ–ª–∞–µ—à—å –∑–∞–¥–∞—á—É –ø–∞—Ä—Å–∏–Ω–≥–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–∫—Å–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–ø–∞—Ä—Å–∏–ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–¢–æ –µ—Å—Ç—å —Ç—ã –±–æ–ª—å—à–µ –≤–µ—Ä–∏—à—å, —á—Ç–æ —è —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥—É –ø–æ–ø–∞—Å—Ç—å –≤ —Ç–æ–ø, —á–µ–º –≤ —Ç–æ, —á—Ç–æ —Ä–∞–Ω–¥–æ–º —Å–µ–π—á–∞—Å –ª—É—á—à–µ –º–æ–¥–µ–ª–µ–π?)</td>\n",
       "      <td>1</td>\n",
       "      <td>–≤–µ—Ä–∏—à—å —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥—É –ø–æ–ø–∞—Å—Ç—å —Ç–æ–ø —Ä–∞–Ω–¥–æ–º –º–æ–¥–µ–ª–µ–π</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "0  –µ—Å–ª–∏ —É —Ç–µ–±—è –Ω–µ—Ç –≤–µ—Ä—Ö–Ω–µ–≥–æ –ø—Ä–µ–¥–µ–ª–∞, —Ñ–∏–∫—Å–µ–¥ –ø–æ–∏–Ω—Ç —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞                                               \n",
       "1  –ê–≥–∞ –∏–ª–∏ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª–∞ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –∏ –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤                                      \n",
       "2  —è –¥—É–º–∞—é, —á—Ç–æ —Å–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∑–æ–∫ –∫ –º–∞–∫—Å–∏–º—É–º—É.                                                         \n",
       "3  –¢—ã –¥–µ–ª–∞–µ—à—å —Ç—É –∑–∞–¥–∞—á—É —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º 20–∫ —Å—Ç—Ä–∞–Ω–∏—Ü?) –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏ –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ —É–∂–µ –±—ã —Å–ø–∞—Ä—Å–∏–ª)           \n",
       "4  –¢–æ –µ—Å—Ç—å —Ç—ã –±–æ–ª—å—à–µ –≤–µ—Ä–∏—à—å, —á—Ç–æ —è —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥—É –ø–æ–ø–∞—Å—Ç—å –≤ —Ç–æ–ø, —á–µ–º –≤ —Ç–æ, —á—Ç–æ —Ä–∞–Ω–¥–æ–º —Å–µ–π—á–∞—Å –ª—É—á—à–µ –º–æ–¥–µ–ª–µ–π?)   \n",
       "\n",
       "   label                                                    text_prepared  \n",
       "0  1      –≤–µ—Ä—Ö–Ω–µ–≥–æ –ø—Ä–µ–¥–µ–ª–∞ —Ñ–∏–∫—Å–µ–¥ –ø–æ–∏–Ω—Ç –ø—Ä–æ–±–ª–µ–º–∞                           \n",
       "1  0      –∞–≥–∞ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª–∞ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤     \n",
       "2  1      –¥—É–º–∞—é —Å–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±–ª–∏–∑–æ–∫ –º–∞–∫—Å–∏–º—É–º—É                               \n",
       "3  0      –¥–µ–ª–∞–µ—à—å –∑–∞–¥–∞—á—É –ø–∞—Ä—Å–∏–Ω–≥–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–∫—Å–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–ø–∞—Ä—Å–∏–ª  \n",
       "4  1      –≤–µ—Ä–∏—à—å —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥—É –ø–æ–ø–∞—Å—Ç—å —Ç–æ–ø —Ä–∞–Ω–¥–æ–º –º–æ–¥–µ–ª–µ–π                   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MxvudPcrk9r3"
   },
   "outputs": [],
   "source": [
    "# from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "# # –ª–µ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞\n",
    "# all_word_str = \" \".join(df[\"text_prepared\"])\n",
    "# all_word_list = all_word_str.split()\n",
    "# all_unique_word = np.unique(all_word_list)\n",
    "# lemmatized_word_dict = {}\n",
    "# lemmatizer = MorphAnalyzer()\n",
    "\n",
    "# for word in all_unique_word:\n",
    "#     lemmatized_word_dict[word] = lemmatizer.normal_forms(word)[0]\n",
    "# func = lambda text: ' '.join([lemmatized_word_dict[word] for word in text.split()])\n",
    "\n",
    "\n",
    "# df['text_prepared'] = df['text_prepared'].apply(func)\n",
    "\n",
    "\n",
    "\n",
    "## —Å–º–æ—Ç—Ä–µ–ª —Ä–∞–Ω–¥–æ–º–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: –º–µ—Å—Ç–∞–º–∏ –Ω–µ –≤—Å–µ –≥–ª–∞–≥–æ–ª—ã –ª–µ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å, –æ—Å–æ–±–µ–Ω–Ω–æ –∞–Ω–≥–ª–∏—Ü–∏–∑–∏–º—ã\n",
    "## –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª –¥—Ä—É–≥–æ–π, —Å—Ç–∞–ª–æ –ø–æ–ª—É—á—à–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NFjbE5qM06AW"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem() \n",
    "func = lambda text: ''.join(mystem.lemmatize(text)).strip()\n",
    "df['text_prepared'] = df['text_prepared'].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "h4WcQFtcsOyc",
    "outputId": "8709325a-cc08-41f1-cf72-3df1e5639f12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_prepared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–µ—Å–ª–∏ —É —Ç–µ–±—è –Ω–µ—Ç –≤–µ—Ä—Ö–Ω–µ–≥–æ –ø—Ä–µ–¥–µ–ª–∞, —Ñ–∏–∫—Å–µ–¥ –ø–æ–∏–Ω—Ç —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞</td>\n",
       "      <td>1</td>\n",
       "      <td>–≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª —Ñ–∏–∫—Å–µ–¥–æ–π –ø–æ–∏–Ω—Ç –ø—Ä–æ–±–ª–µ–º–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ê–≥–∞ –∏–ª–∏ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª–∞ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –∏ –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td>\n",
       "      <td>0</td>\n",
       "      <td>–∞–≥–∞ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª –ø–æ–¥–æ–±–Ω—ã–π —Å–∞–π—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω–∞ –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—è –¥—É–º–∞—é, —á—Ç–æ —Å–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∑–æ–∫ –∫ –º–∞–∫—Å–∏–º—É–º—É.</td>\n",
       "      <td>1</td>\n",
       "      <td>–¥—É–º–∞—Ç—å —Å–∫–æ—Ä—ã–π –¥–æ–ª–∂–Ω—ã–π –±–ª–∏–∑–∫–∏–π –º–∞–∫—Å–∏–º—É–º</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢—ã –¥–µ–ª–∞–µ—à—å —Ç—É –∑–∞–¥–∞—á—É —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º 20–∫ —Å—Ç—Ä–∞–Ω–∏—Ü?) –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏ –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ —É–∂–µ –±—ã —Å–ø–∞—Ä—Å–∏–ª)</td>\n",
       "      <td>0</td>\n",
       "      <td>–¥–µ–ª–∞—Ç—å –∑–∞–¥–∞—á–∞ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∫—Å–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å —Å–ø–∞—Ä—Å–∏—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–¢–æ –µ—Å—Ç—å —Ç—ã –±–æ–ª—å—à–µ –≤–µ—Ä–∏—à—å, —á—Ç–æ —è —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥—É –ø–æ–ø–∞—Å—Ç—å –≤ —Ç–æ–ø, —á–µ–º –≤ —Ç–æ, —á—Ç–æ —Ä–∞–Ω–¥–æ–º —Å–µ–π—á–∞—Å –ª—É—á—à–µ –º–æ–¥–µ–ª–µ–π?)</td>\n",
       "      <td>1</td>\n",
       "      <td>–≤–µ—Ä–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ –º–æ—á—å –ø–æ–ø–∞–¥–∞—Ç—å —Ç–æ–ø —Ä–∞–Ω–¥ –º–æ–¥–µ–ª—å</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "0  –µ—Å–ª–∏ —É —Ç–µ–±—è –Ω–µ—Ç –≤–µ—Ä—Ö–Ω–µ–≥–æ –ø—Ä–µ–¥–µ–ª–∞, —Ñ–∏–∫—Å–µ–¥ –ø–æ–∏–Ω—Ç —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞                                               \n",
       "1  –ê–≥–∞ –∏–ª–∏ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª–∞ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω –∏ –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤                                      \n",
       "2  —è –¥—É–º–∞—é, —á—Ç–æ —Å–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∑–æ–∫ –∫ –º–∞–∫—Å–∏–º—É–º—É.                                                         \n",
       "3  –¢—ã –¥–µ–ª–∞–µ—à—å —Ç—É –∑–∞–¥–∞—á—É —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º 20–∫ —Å—Ç—Ä–∞–Ω–∏—Ü?) –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –∏ –±–µ–∑ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ —É–∂–µ –±—ã —Å–ø–∞—Ä—Å–∏–ª)           \n",
       "4  –¢–æ –µ—Å—Ç—å —Ç—ã –±–æ–ª—å—à–µ –≤–µ—Ä–∏—à—å, —á—Ç–æ —è —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥—É –ø–æ–ø–∞—Å—Ç—å –≤ —Ç–æ–ø, —á–µ–º –≤ —Ç–æ, —á—Ç–æ —Ä–∞–Ω–¥–æ–º —Å–µ–π—á–∞—Å –ª—É—á—à–µ –º–æ–¥–µ–ª–µ–π?)   \n",
       "\n",
       "   label                                                   text_prepared  \n",
       "0  1      –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª —Ñ–∏–∫—Å–µ–¥–æ–π –ø–æ–∏–Ω—Ç –ø—Ä–æ–±–ª–µ–º–∞                          \n",
       "1  0      –∞–≥–∞ –ø–∞—Ä–∫–∏–Ω–≥ –ø—É–ª –ø–æ–¥–æ–±–Ω—ã–π —Å–∞–π—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω–∞ –¥—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä        \n",
       "2  1      –¥—É–º–∞—Ç—å —Å–∫–æ—Ä—ã–π –¥–æ–ª–∂–Ω—ã–π –±–ª–∏–∑–∫–∏–π –º–∞–∫—Å–∏–º—É–º                          \n",
       "3  0      –¥–µ–ª–∞—Ç—å –∑–∞–¥–∞—á–∞ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∫—Å–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å —Å–ø–∞—Ä—Å–∏—Ç—å  \n",
       "4  1      –≤–µ—Ä–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ –º–æ—á—å –ø–æ–ø–∞–¥–∞—Ç—å —Ç–æ–ø —Ä–∞–Ω–¥ –º–æ–¥–µ–ª—å                    "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHMH7VAqNJoo"
   },
   "source": [
    "## –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bHFfBN9bSZk5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text_prepared']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, \n",
    "                                                    test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynpYPH4nO0Ul"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "827ZWyubmTI8",
    "outputId": "ec2bdac0-6a7e-42f3-f9db-0c3928f0e4bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 1166)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# –Ω–∞—á–Ω–µ–º —Å –∫–ª–∞—Å—Å–∏–∫–∏\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=10, \n",
    "    max_df=50, \n",
    "    ngram_range=(1, 5)\n",
    ")\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeIkOjxZO2wT"
   },
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O6_f_fmMO4Hh"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# –ø–æ–ø—Ä–æ–±—É–µ–º —á—É—Ç—å –±–æ–ª–µ–µ-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "# —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –ø—Ä–æ—Å—Ç–æ —É—Å—Ä–µ–¥–Ω–∏–º\n",
    "docs = [doc.split() for doc in X_train]\n",
    "w2v = Word2Vec(sentences=docs, vector_size=100, window=4, sg=0, min_count=1, epochs=100, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_trans_word(word, w2v=w2v):\n",
    "    '''\n",
    "    –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
    "    '''\n",
    "    try:\n",
    "        return w2v.wv.get_vector(word)\n",
    "    except:\n",
    "        return np.zeros(100)\n",
    "\n",
    "def w2v_trans_doc(doc, w2v=w2v):\n",
    "    '''\n",
    "    —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è pd.Series –≤ –≤–µ–∫—Ç–æ—Ä w2v —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ apply\n",
    "    '''\n",
    "    return np.mean([w2v_trans_word(word, w2v) for word in doc.split()], axis=0)\n",
    "    \n",
    "def w2v_vectorizer(X_test, w2v=w2v):\n",
    "    '''\n",
    "    —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è pd.Series –≤ –≤–µ–∫—Ç–æ—Ä w2v —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ apply,\n",
    "    –∞ —Ç–∞–∫–∂–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π, –≥–¥–µ –æ–∫–∞–∑–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "    '''\n",
    "    return np.vstack([np.zeros(100) if np.isnan(np.std(doc)) else doc for doc in \n",
    "                     X_test.apply(w2v_trans_doc, w2v).tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WDHpNk6CWyjW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9923, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v = w2v_vectorizer(X_train)\n",
    "X_test_w2v = w2v_vectorizer(X_test)\n",
    "\n",
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qje0rz6ZQkYB"
   },
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MJf_RAjDKXF8"
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "# –ø–æ–ø—Ä–æ–±—É–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ü–µ–ª–∏–∫–æ–º\n",
    "# —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ w2v –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å —Å–∏–ª—å–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤\n",
    "def tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "data_for_training = list(tagged_document(docs))\n",
    "d2v_model = doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=100, seed=0)\n",
    "d2v_model.build_vocab(data_for_training)\n",
    "d2v_model.train(data_for_training, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLhd8PkBMCw2",
    "outputId": "b3cae0a1-7389-4bbd-edd2-62ff1d53fb4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_d2v = np.stack([d2v_model.infer_vector(doc.split()) for doc in X_train])\n",
    "X_test_d2v = np.stack([d2v_model.infer_vector(doc.split()) for doc in X_test])\n",
    "X_train_d2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7QOt-SWUhlp"
   },
   "source": [
    "# –ú–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TUFcsvfZ69G"
   },
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DCy0sdd1mTs2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=3000, random_state=0) #, penalty='l1', C=1, solver='saga')\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "## —á—É—Ç—å-—á—É—Ç—å –∏–≥—Ä–∞–ª —Å –æ—Ç—Å–µ–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–Ω—Å–∞–π—Ç–æ–≤ –Ω–µ –±—ã–ª–æ –æ—Å–æ–±–æ\n",
    "# k = clf.coef_.shape[1]\n",
    "# k_zero = (clf.coef_==0).sum()\n",
    "# print(f'{k_zero} –∏–∑ {k} –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∑–∞–Ω—É–Ω–∏–ª–æ—Å—å')\n",
    "# print(f'–æ—Å—Ç–∞–ª–æ—Å—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ {k-k_zero}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1RAC8j4RVOOM"
   },
   "outputs": [],
   "source": [
    "Xs = [\n",
    "    [X_train_vec, X_test_vec],\n",
    "    [X_train_w2v, X_test_w2v],\n",
    "    [X_train_d2v, X_test_d2v],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bO77raoOV3h_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "XsTuWk9AnsZM",
    "outputId": "80aa79b4-cf9a-4fed-96c8-32a1239368da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.775773</td>\n",
       "      <td>0.729141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.764990</td>\n",
       "      <td>0.766626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec</th>\n",
       "      <td>0.723773</td>\n",
       "      <td>0.734381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train      test\n",
       "TF-IDF    0.775773  0.729141\n",
       "word2vec  0.764990  0.766626\n",
       "doc2vec   0.723773  0.734381"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = []\n",
    "clfs = {}\n",
    "for i, preprocess in enumerate(Xs):\n",
    "    clf.fit(Xs[i][0], y_train)\n",
    "    clfs[i] = clf\n",
    "    acc.append([\n",
    "        accuracy_score(y_train, clf.predict(Xs[i][0])),\n",
    "        accuracy_score(y_test, clf.predict(Xs[i][1]))\n",
    "        ])\n",
    "    \n",
    "# —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–≥ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "\n",
    "acc_clf = pd.DataFrame(acc, index=['TF-IDF', 'word2vec', 'doc2vec'], columns=['train', 'test'])\n",
    "acc_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uQaNbZae64p"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "B-PrtK2Ke8oY"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=30)\n",
    "scaler = StandardScaler(with_mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "C3aIC1VYe_x8",
    "outputId": "33f9d514-f4f2-487c-db48-4c0aa7da93f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.673687</td>\n",
       "      <td>0.641677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.761262</td>\n",
       "      <td>0.742846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec</th>\n",
       "      <td>0.650811</td>\n",
       "      <td>0.659412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train      test\n",
       "TF-IDF    0.673687  0.641677\n",
       "word2vec  0.761262  0.742846\n",
       "doc2vec   0.650811  0.659412"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = []\n",
    "for i, preprocess in enumerate(Xs):\n",
    "    scaler.fit(Xs[i][0])\n",
    "    knn.fit(scaler.transform(Xs[i][0]), y_train)\n",
    "    acc.append([\n",
    "        accuracy_score(y_train, knn.predict(scaler.transform(Xs[i][0]))),\n",
    "        accuracy_score(y_test, knn.predict(scaler.transform(Xs[i][1])))\n",
    "        ])\n",
    "    \n",
    "# —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã KNN\n",
    "\n",
    "acc_knn = pd.DataFrame(acc, index=['TF-IDF', 'word2vec', 'doc2vec'], columns=['train', 'test'])\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBU_cNvvZ9Je"
   },
   "source": [
    "## CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xV4OL-MfZLz-"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat = CatBoostClassifier(metric_period=500, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "YCCZ8MgiZPl2",
    "outputId": "e6bb1923-84e5-4550-e7aa-26212e33988e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.027447\n",
      "0:\tlearn: 0.6923653\ttotal: 64.5ms\tremaining: 1m 4s\n",
      "500:\tlearn: 0.5798352\ttotal: 2.92s\tremaining: 2.9s\n",
      "999:\tlearn: 0.5269313\ttotal: 5.8s\tremaining: 0us\n",
      "Learning rate set to 0.027447\n",
      "0:\tlearn: 0.6869118\ttotal: 7ms\tremaining: 7s\n",
      "500:\tlearn: 0.3787070\ttotal: 2.93s\tremaining: 2.92s\n",
      "999:\tlearn: 0.2730399\ttotal: 5.8s\tremaining: 0us\n",
      "Learning rate set to 0.027447\n",
      "0:\tlearn: 0.6906175\ttotal: 7.2ms\tremaining: 7.19s\n",
      "500:\tlearn: 0.4481818\ttotal: 2.89s\tremaining: 2.88s\n",
      "999:\tlearn: 0.3319108\ttotal: 5.81s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.763580</td>\n",
       "      <td>0.706973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.922906</td>\n",
       "      <td>0.767432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec</th>\n",
       "      <td>0.914945</td>\n",
       "      <td>0.724305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train      test\n",
       "TF-IDF    0.763580  0.706973\n",
       "word2vec  0.922906  0.767432\n",
       "doc2vec   0.914945  0.724305"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = []\n",
    "cats = {}\n",
    "for i, preprocess in enumerate(Xs):\n",
    "    cat.fit(Xs[i][0], y_train)\n",
    "    cats[i] = cat\n",
    "    acc.append([\n",
    "        accuracy_score(y_train, cat.predict(Xs[i][0])),\n",
    "        accuracy_score(y_test, cat.predict(Xs[i][1]))\n",
    "        ])\n",
    "    \n",
    "# —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞—Ç–±—É—Å—Ç–∞\n",
    "\n",
    "acc_cat = pd.DataFrame(acc, index=['TF-IDF', 'word2vec', 'doc2vec'], columns=['train', 'test'])\n",
    "acc_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.69      1104\n",
      "           1       0.75      0.76      0.75      1377\n",
      "\n",
      "    accuracy                           0.72      2481\n",
      "   macro avg       0.72      0.72      0.72      2481\n",
      "weighted avg       0.72      0.72      0.72      2481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, cat.predict(Xs[i][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7WwoRnAhkTf"
   },
   "source": [
    "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –Ω–∞—á–∞–ª–∞ –±—ã–ª –≤—ã–±—Ä–∞–Ω –ø—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥, –∞ –∏–º–µ–Ω–Ω–æ TF-IDF –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–≤—è–∑–∫–µ —Å –ª–æ–≥. —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π. –£–∂–µ –æ–Ω –ø–æ–∫–∞–∑–∞–ª –Ω–µ–ø–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –æ–¥–Ω–∞–∫–æ –¥–∞–ª–µ–µ –±—ã–ª–∞ –ø–æ–ø—ã—Ç–∫–∞ –µ–≥–æ —É–ª—É—á—à–∏—Ç—å. –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –±—ã–ª–∏ —Å–¥–µ–ª–∞–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–º–æ—â—å—é word2vec, –∞ –∑–∞—Ç–µ–º –∏ —Å –ø–æ–º–æ—â—å—é doc2vec. –ê –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±—ã–ª–∏ –≤–∑—è—Ç—ã KNN –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥. KNN –≤—ã–±—Ä–∞–Ω –∏–∑ —Ç–æ–π –ª–æ–≥–∏–∫–∏, —á—Ç–æ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –æ–Ω –º–æ–∂–µ—Ç —Å–µ–±—è —Ö–æ—Ä–æ—à–æ –ø–æ–∫–∞–∑–∞—Ç—å, –∞ –±—É—Å—Ç–∏–Ω–≥ –≤ —Ü–µ–ª–æ–º —á–∞—Å—Ç–æ —Ö–æ—Ä–æ—à–æ —Å–µ–±—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç.\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ accuracy, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å—ã —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã. –ü–æ–ª–Ω–æ—Ç–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –æ–±–æ–∏–º –∫–ª–∞—Å—Å–∞–º –±—ã–ª–∞ –≤ —Ü–µ–ª–æ–º —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞, –ø–æ—ç—Ç–æ–º—É accuracy –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ. –ü—Ä–∏–º–µ—Ä —Å –ø–æ–ª–Ω–æ—Ç–æ–π –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –ø—Ä–∏–≤–µ–¥–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ —É –±—É—Å—Ç–∏–Ω–≥–∞ (–ø–æ –æ—Å—Ç–∞–ª—å–Ω—ã–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º –±—ã–ª–∞ –ø–æ—Ö–æ–∂–∞—è –¥–∏–Ω–∞–º–∏–∫–∞).\n",
    "\n",
    "–ö–∞–∫ –ø–æ–∫–∞–∑–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ word2vec –æ–∫–∞–∑–∞–ª–∏—Å—å –ª—É—á—à–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ doc2vec, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é TF-IDF –æ–∫–∞–∑–∞–ª–∞—Å—å —Ö—É–∂–µ –≤—Å–µ—Ö. –î–∏–Ω–∞–º–∏–∫–∞ –±—ã–ª–∞ —Å—Ö–æ–∂–∞ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π. –õ—É—á—à–µ –≤—Å–µ–≥–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª —Å–µ–±—è –±—É—Å—Ç–∏–Ω–≥, –Ω–æ –ª–æ–≥. —Ä–µ–≥—Ä–µ—Å—Å–∏—è –≤—Å–µ–≥–æ –Ω–µ–º–Ω–æ–≥–æ —Ö—É–∂–µ, –º–µ—Ç–æ–¥ –±–ª–∏–∂. —Å–æ—Å–µ–¥–µ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ –≤—ã–±—Ä–∞–Ω–∞ –ª–æ–≥. —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∑–∞ —Å—á—ë—Ç –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–µ–¥–∏–∫—Ç–∞. –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–∑—è—Ç word2vec.\n",
    "\n",
    "–ù–∏–∂–µ –ø–∞—Ä–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Ç–µ–º, –∫–∞–∫ –¥–µ–ª–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\n",
    "\n",
    "–ü—Ä–æ –∞–ø–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "TVHcmOaIhOT-",
    "outputId": "066b6d95-8560-4f99-b28a-d2accac3a032",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">LR</th>\n",
       "      <th colspan=\"2\" halign=\"left\">KNN</th>\n",
       "      <th colspan=\"2\" halign=\"left\">CB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.775773</td>\n",
       "      <td>0.729141</td>\n",
       "      <td>0.673687</td>\n",
       "      <td>0.641677</td>\n",
       "      <td>0.763580</td>\n",
       "      <td>0.706973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.764990</td>\n",
       "      <td>0.766626</td>\n",
       "      <td>0.761262</td>\n",
       "      <td>0.742846</td>\n",
       "      <td>0.922906</td>\n",
       "      <td>0.767432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2vec</th>\n",
       "      <td>0.723773</td>\n",
       "      <td>0.734381</td>\n",
       "      <td>0.650811</td>\n",
       "      <td>0.659412</td>\n",
       "      <td>0.914945</td>\n",
       "      <td>0.724305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                LR                 KNN                  CB          \n",
       "             train      test     train      test     train      test\n",
       "TF-IDF    0.775773  0.729141  0.673687  0.641677  0.763580  0.706973\n",
       "word2vec  0.764990  0.766626  0.761262  0.742846  0.922906  0.767432\n",
       "doc2vec   0.723773  0.734381  0.650811  0.659412  0.914945  0.724305"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat([acc_clf,acc_knn,acc_cat], keys=['LR', 'KNN', 'CB'], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å–æ—Ö—Ä–∞–Ω–∏–º –ª–æ–≥. —Ä–µ–≥\n",
    "final = clfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(final, f)\n",
    "    \n",
    "with open('w2v.pickle', 'wb') as f:\n",
    "    pickle.dump(w2v, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('w2v.pickle', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(doc, w2v=w2v):\n",
    "    vec = w2v_vectorizer(pd.Series(doc), w2v)\n",
    "    proba = model.predict_proba(vec)[:,1][0]\n",
    "\n",
    "    if proba>0.5:\n",
    "        return f'–≠—Ç–æ —á–∞—Ç –ø–æ Data Science —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {round(proba*100)}% : \\t {doc}'\n",
    "    else:\n",
    "        return f'–≠—Ç–æ —á–∞—Ç –ø–æ Python —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {round((1-proba)*100)}% : \\t\\t {doc}'\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_simple = pd.Series([\n",
    "    '–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∫–ª–∞—Å—Å –ª—è–º–±–¥–∞',\n",
    "    '–ª–æ–≥—Ä–µ–≥ –±—É—Å—Ç–∏–Ω–≥ —ç–º–±–µ–¥–∏–Ω–Ω–≥–∏ —Ç–æ–ø'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≠—Ç–æ —á–∞—Ç –ø–æ Python —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 93% : \t\t –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∫–ª–∞—Å—Å –ª—è–º–±–¥–∞\n",
      "–≠—Ç–æ —á–∞—Ç –ø–æ Data Science —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 100% : \t –ª–æ–≥—Ä–µ–≥ –±—É—Å—Ç–∏–Ω–≥ —ç–º–±–µ–¥–∏–Ω–Ω–≥–∏ —Ç–æ–ø\n"
     ]
    }
   ],
   "source": [
    "for x in X_test_simple:\n",
    "    print(get_pred(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.811px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
